{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e133a50d-1659-4c8a-930e-ad85fff5f718",
   "metadata": {},
   "source": [
    "# Experimenting with VGAE Code\n",
    "\n",
    "Code source: https://github.com/DaehanKim/vgae_pytorch\n",
    "Paper reference: \"Variational Graph Auto-Encoders\" by Thomas N. Kipf and Max Welling, 2016\n",
    "\n",
    "## To figure out: \n",
    "- [ ] how do they pre-process their data? What form does their input data take? \n",
    "- [ ] how does GAE and GVAE work? can I implement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd037ad-10e2-4a51-971c-3f87862c5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb15534e-9506-487f-bd9b-9fdc62554215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pyprojroot import here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "# from input_data import load_data\n",
    "# from preprocessing import *\n",
    "# import args\n",
    "# import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe53ca4-247c-4275-aae4-80e9e8b6a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = here(project_files=[\".here\"])\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c29e718-da1e-4ad7-89ac-7a455d0ab2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6d2148-31f1-4a1d-af32-68bf502b8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Function builds a list of indices from a given filename.\n",
    "    \n",
    "    Args:\n",
    "    filename (str): filename (including extension)\n",
    "    \n",
    "    Returns:\n",
    "    index (list of int): list of indices from filename\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip())) #.strip removes extra whitespace\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5aab63e-b60f-4792-8517-92bd532beacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    \"\"\"Function loads data from different citation network datasets. Assumes all datasets contain\n",
    "    4 files with extensions .x, .tx, .allx, .graph. This function extracts the data from the 4 files\n",
    "    and uses it to generate an adjacency matrix and feature vectors for each node. The adjacency matrix \n",
    "    is for one large citation network graph. \n",
    "    \n",
    "    Args: \n",
    "    dataset (str): name of the dataset to load\n",
    "    \n",
    "    Returns:\n",
    "    adj \n",
    "    \"\"\"\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}/data/ind.{}.{}\".format(root, dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "                \n",
    "    # graph is a dict (default dict from collections module)\n",
    "    # each key is a node, each value is a list of the adjacent nodes\n",
    "    \n",
    "    # x is a compressed sparse row matrix (scipy)\n",
    "    # each entry in x indicates where there are connections between papers\n",
    "    # what is the difference between x, tx and allx?\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "#     print('graph', type(graph))\n",
    "#     print('graph', graph)\n",
    "#     print(\"x\", x[0])\n",
    "#     print(\"x type\", type(x))\n",
    "#     print(\"tx\", tx)\n",
    "#     print(\"tx type\", type(tx))\n",
    "#     print(\"allx\", allx)\n",
    "#     print(\"allx type\", type(allx))\n",
    "    \n",
    "    # test_idx_reorder is the list of file indices out of order\n",
    "    # test_idx_range is the sorted list of file indices\n",
    "    test_idx_reorder = parse_index_file(\"{}/data/ind.{}.test.index\".format(root, dataset))\n",
    "#     print(\"test Index reorder\", test_idx_reorder)\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "#     print(\"test index range\", test_idx_range)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    # lil is a list of lists, another way to represent adjacency information\n",
    "    # why are we using allx and tx and not x? \n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "#     print(\"features\", features[0][0])\n",
    "#     print(\"features in test_idx_range\", features[test_idx_range, :])\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :] # what is this line doing? \n",
    "#     print(\"features in test_idx_reorder\", features[test_idx_reorder, :])\n",
    "\n",
    "    # build an adjacency matrix which is a compressed sparse row matrix\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079c62c8-a2bb-4de5-b0fe-7eeef40b8640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "adj, features = load_data(dataset)\n",
    "\n",
    "# print(adj.shape)\n",
    "# print(type(adj))\n",
    "# print(adj)\n",
    "\n",
    "# adj is a sparse matrix (scipy datatype) that contains all of the information provided in graph, see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef78264-7aed-406c-b12f-860b2e2e799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "# is this really doing anything? hard to check...\n",
    "# why are we removing diagonal entries? \n",
    "# the paper says assume every diagonal entry is 1, i.e. nodes are self-connected\n",
    "\n",
    "adj_orig = adj\n",
    "# print(\"before mods\", adj_orig)\n",
    "# .diagonal returns the values of the diagonal of adj_orig as an array\n",
    "# np.newaxis adds a dimension to the array\n",
    "# print(\"adj_orig diagonal\", adj_orig.diagonal()[np.newaxis, :].shape)\n",
    "# print(\"adj_orig dia_matrix\", sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape))\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "# print(\"after subtraction\", adj_orig)\n",
    "adj_orig.eliminate_zeros()\n",
    "# print(\"after removing zeros\", adj_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a3df3-bb59-43fc-9e55-af027a733f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: understand these functions and what they're doing\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Function obtains the coordinates, values and shape from a sparse matrix\n",
    "    required to build a COO matrix representation. \n",
    "    \n",
    "    Args: \n",
    "    sparse_mx (COO matrix): The sparse matrix to be converted\n",
    "    \n",
    "    Returns: \n",
    "    coords (numpy.ndarray): The coordinates of the values in the adjacency matrix\n",
    "    values (numpy.ndarray): The entries in the adjacency matrix\n",
    "    shape (tuple): The shape of the adjacency matrix\n",
    "    \"\"\"\n",
    "#     print(\"type of sparse_mx\", type(sparse_mx))\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    \"\"\"Function takes adjacency matrix as input and returns the normalized adjacency matrix. \n",
    "    The normalized adjacency matrix is symmetric and is normalized on a row-by-row basis.\n",
    "    \n",
    "    Args: \n",
    "    adj (compressed sparse row matrix): adjacency matrix (raw)\n",
    "    \n",
    "    Returns: \n",
    "    adj_normalized (tuple): the normalized adjacency matrix, given as a tuple containing\n",
    "        (coords, values, shape) to be used to build a COO matrix\n",
    "    \"\"\"\n",
    "#     print(\"adj input\", adj)\n",
    "#     print(\"adj input type\", type(adj))\n",
    "    # coo_matrix((data, (row, col)), shape=(4, 4)).toarray()\n",
    "    adj = sp.coo_matrix(adj)\n",
    "#     print(\"adj in coo matrix form\", adj)\n",
    "    \n",
    "#     print(\"eye\", sp.eye(adj.shape[0]))\n",
    "    # maybe this is adding 1's to the diagonal? \n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    \n",
    "    # I think this paper is doing row-based normalization?\n",
    "    # I think that column-based would be equivalent? \n",
    "    # why not just normalize over the entire array?\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    \n",
    "    # this is A_norm = D^(1/2) * A * D^(1/2) \n",
    "    # D is the degree matrix\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    \n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696a0c7a-cdc2-4cab-b7d2-9a9592c3d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj_orig)\n",
    "print(len(adj_norm))\n",
    "# print(adj_norm[0]) # coords\n",
    "# print(adj_norm[1]) # values\n",
    "# print(adj_norm[2]) # shape\n",
    "print(type(adj_norm[0])) # coords\n",
    "print(type(adj_norm[1])) # values\n",
    "print(type(adj_norm[2])) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f4aa663-8d28-414b-9943-fdad872467a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes 2708\n",
      "feature coords [[   0   19]\n",
      " [   0   81]\n",
      " [   0  146]\n",
      " ...\n",
      " [2707 1328]\n",
      " [2707 1412]\n",
      " [2707 1414]]\n",
      "feature values [1. 1. 1. ... 1. 1. 1.]\n",
      "feature shape (2708, 1433)\n",
      "num features 1433\n"
     ]
    }
   ],
   "source": [
    "num_nodes = adj.shape[0] # adj is still a numpy array\n",
    "print(\"num nodes\", num_nodes)\n",
    "\n",
    "# print(\"features\", features)\n",
    "\n",
    "features_coords, features_values, features_shape = sparse_to_tuple(features.tocoo())\n",
    "print(\"feature coords\", features_coords)\n",
    "print(\"feature values\", features_values)\n",
    "print(\"feature shape\", features_shape) # shape is (num samples, num features)\n",
    "\n",
    "\n",
    "num_features = features_shape[1]\n",
    "print(\"num features\", num_features)\n",
    "# features_nonzero = features[1].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:glm_env]",
   "language": "python",
   "name": "conda-env-glm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
